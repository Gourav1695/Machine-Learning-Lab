# -*- coding: utf-8 -*-
"""assign4_part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xg6XXYkEddD_IdZc6NU3Xg3MF5rxzKEb

# Assignment 4 Part 1

Name: Gourav Kumar Shaw

Enrollment Number: 2020CSB010

Machine Learning Lab

## Task 1

Download Titanic Dataset (https://www.kaggle.com/heptapod/titanic/version/1#) and do initial pre-processing including normalization, na or zero column handling, train test split, and others (Write an explanation of each in the report).
"""

import pandas as pd

FILE_PATH = "/content/drive/MyDrive/ML_DRIVE/Assign_3/train_and_test2.csv"

# dropping all the na rows
titanic_df = pd.read_csv(FILE_PATH).dropna()

titanic_df

from google.colab import drive
drive.mount('/content/drive')

titanic_df.columns

# all the zero column are not useful (kaggle saying all zero)
# so ignoring them

# also dropping "Passengerid" cause using pandas internal
# 0-index id

titanic_df = titanic_df[
    filter(
        lambda colName: "zero" not in colName,
        titanic_df.columns
    )
]
titanic_df = titanic_df.drop("Passengerid", axis=1)
titanic_df

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler


def one_hot_encode(X: "pd.DataFrame", col_name: "str") -> "pd.DataFrame":
    """
    alters X by one-hot-encoding the values of the col_name
    using OneHotEncoder(), returns the altered DataFrame
    """
    encoder = OneHotEncoder()

    encoded_df = pd.DataFrame(
        encoder.fit_transform(X[[col_name]]).toarray(),
        index=X.index,
        columns=encoder.get_feature_names_out()
    )

    X = X.join(encoded_df)
    X = X.drop(col_name, axis=1)

    return X


def standardize(df: "pd.DataFrame", col_name: "str") -> "pd.DataFrame":
    """
    alters df by standardizing the values of the col_name
    using StandardScaler(), returns the altered DataFrame
    """
    scaler = StandardScaler()

    df[[col_name]] = pd.DataFrame(
        data=scaler.fit_transform(df[[col_name]]),
        index=df.index,
        columns=[col_name]
    )
    return df

# Pclass has value ranging from 1 to 3 (doing OneHotEncoding)
# Sex has value ranging from 0 to 2 (doing OneHotEncoding)
# Embarked has value ranging from 0 to 3 (doing OneHotEncoding)
# One-hot encoding is a technique used in data preprocessing, especially in the context of machine learning,
# to represent categorical data as binary vectors.
columns_to_encode = ["Pclass", "Embarked", "Sex"]

for column in columns_to_encode:
    titanic_df = one_hot_encode(titanic_df, column)

titanic_df

# Age, Fare, sibsp, Parch needs to be standardized as their values
# are not in the range of 0 to 1

columns_to_standardize = ['Age', "Fare", 'sibsp', "Parch"]

for column in columns_to_standardize:
    titanic_df = standardize(titanic_df, column)

titanic_df

"""## Task 2

Train the SVM using the below kernels with parameters, present the support vectors in the table of the comparison of the model along with accuracy.
1. Linear
2. Polynomial:  where degree d is set to 2, 3 and  5
3. RBF
4. Sigmoid
"""

from sklearn.svm import SVC
#rbf-> K(x,x′)=exp(−∥x−x′∥^2 /​2σ^2)
#linear svm-> f(x)=sign(w⋅x+b)

def trainSVC(
    X_train: "pd.DataFrame",
    X_test: "pd.DataFrame",
    y_train: "pd.DataFrame",
    y_test: "pd.DataFrame",
    kernel: "str",
    degree: "int" = 3,
    return_model = False
    ):
    """
    degree is ignored if kernel is not 'poly'
    """
    model = SVC(kernel=kernel, degree=degree)
    model.fit(X_train,y_train.iloc[:,0])
    accuracy = model.score(X_test, y_test)

    if return_model:
        return model

    return [kernel, degree if kernel=='poly' else 'None', accuracy, model.support_vectors_]

from sklearn.model_selection import train_test_split

X = titanic_df.drop('2urvived', axis=1)
y = titanic_df[['2urvived']]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

trainSVC(X_train, X_test, y_train, y_test, 'linear')

configs = [
    ['linear'],
    ['poly', 2],
    ['poly', 3],
    ['poly', 5],
    ['rbf'],
    ['sigmoid']
]
# sigmoid-> f(x)=tanh(α⋅(x⋅x′)+β)
data = [
    trainSVC(X_train, X_test, y_train, y_test, *config)
    for config in configs
]

pd.DataFrame(data, columns=['kernel', 'degree (only for poly)', 'accuracy', 'support vectors'])

#     Import necessary libraries:
#         from sklearn.inspection import DecisionBoundaryDisplay: This imports the DecisionBoundaryDisplay
#         class from scikit-learn, which is used for visualizing decision boundaries.
#         import matplotlib.pyplot as plt: This imports the matplotlib library for creating plots.

#     Define a function plot_decision_boundary:
#         This function takes two main arguments:
#             kernel: The type of kernel function to be used in the SVM (e.g., 'linear', 'poly', etc.).
#             degree (optional): The degree of the polynomial kernel (only relevant if kernel is 'poly').
#             The default value is 1.
#         It assumes the existence of a DataFrame called titanic_df and a Series called y.

#         Inside the function:
#         It extracts the first two columns of the DataFrame titanic_df as the feature data and assigns it to the variable X.
#         It creates an SVM model using the specified kernel and degree and fits it to the data.
#         This model is stored in the model variable.
#         It then uses DecisionBoundaryDisplay.from_estimator to create a decision boundary display.
#         This function visualizes the decision boundary produced by the SVM model.
#   ----> The alpha parameter sets the transparency of the decision boundary, and eps controls the step size for
#         decision boundary computation.
#         It plots the feature data points as a scatter plot, coloring them based on the values in the y Series.
#         It sets the plot's title depending on the kernel type ('linear', 'poly', etc.).
#         Finally, it displays the plot using plt.show().

#     A loop runs over some configs (not provided in the code snippet). Presumably, configs is a list of tuples
#     containing kernel type and optionally polynomial degrees (e.g., [('linear',), ('poly', 2), ('rbf',)]).
#     For each configuration in configs, it calls the plot_decision_boundary function, passing the kernel and
#     degree (if applicable) as arguments.

# Overall, this code is meant to create and display decision boundaries for different SVM models with various kernel
# types and degrees, allowing you to visually inspect how the models separate data points in a 2D feature space.
# However, the configs list is not defined in the provided code, so you would need to define it elsewhere in your script.

"""## Task 3

Take only two features from the dataset and train the models with the same parameters and plot the graphs to show the boundaries. Also, create a custom kernel function of your own using a mathematical function for suggestion Logarithmic or Tangent function.
"""

from sklearn.inspection import DecisionBoundaryDisplay
import matplotlib.pyplot as plt

def plot_decision_boundary(kernel: str, degree = 1):
    X = titanic_df.iloc[:, 0:2]
    model = SVC(kernel=kernel, degree=degree).fit(X, y.iloc[:, 0])

    disp = DecisionBoundaryDisplay.from_estimator(model, X, alpha=0.5, eps=0.1)
    disp.ax_.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y.iloc[:, 0])
    if(kernel == 'poly'):
        plt.title(f"{kernel} {degree}")
    else:
        plt.title(f"{kernel}")
    plt.show()

for config in configs:
    plot_decision_boundary(*config)

import numpy as np

def my_kernel(X:"np.array", Y: "np.array") -> "np.array":
    M = np.array([[1, 0], [0, 1]])
    return np.dot(np.dot(np.tan(X), M), Y.T)

X_sub = titanic_df.iloc[:, 0:2]

model = SVC(kernel=my_kernel).fit(X_sub, y.iloc[:, 0])
disp = DecisionBoundaryDisplay.from_estimator(model, X_sub, alpha=0.5, eps=0.1)
disp.ax_.scatter(X_sub.iloc[:, 0], X_sub.iloc[:, 1], c=y.iloc[:, 0])
plt.title('Tan Kernel')
plt.show()

"""## Task 4

For RBF kernel vary the control parameter C with a binary search technique to reach an optimal C value. Plot the graph for validation accuracy. Using this, mention the situation of overfitting and underfitting. Set Gamma to 0.5. Create a function for the whole process. [Maximum 20 runs]
"""

def binary_search_C(
    X_train: "pd.DataFrame",
    X_val: "pd.DataFrame",
    y_train: "pd.DataFrame",
    y_val: "pd.DataFrame"):
    right = 0.1
    left = 0
    max_acc = 0
    acc = 0.1
    accuracies = []
    Cs = []
    while acc >= max_acc:

        left = right
        right *= 2
        model = SVC(kernel='rbf',gamma = 0.5 ,C = right)\
            .fit(X_train, y_train.iloc[:, 0])
        max_acc = max(max_acc, acc)
        acc = model.score(X_val, y_val)
        print(left, right, acc)

        accuracies.append(acc)
        Cs.append(right)

    plt.plot(Cs, accuracies, 'o-', label="Forward Exponentiation")
    Cs = []
    accuracies = []

    print("phase2")

    while left <= right:
        mid = (left + right)/2
        model = SVC(kernel='rbf',C = mid, gamma = 0.5).fit(X_train, y_train.iloc[:, 0])
        acc = model.score(X_val, y_val)

        accuracies.append(acc)
        Cs.append(mid)

        print(left, mid ,right, acc)

        if acc < max_acc:
            right = mid - 0.0001
        elif acc > max_acc:
            left = mid + 0.0001
            max_acc = acc
        else:
            break

    plt.plot(Cs, accuracies, 'o-', label="Binary Search")
    plt.title("Exponentiation Propagation and Binary Search to find best C")
    plt.xlabel("C")
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()



    return mid

X = titanic_df.iloc[:, 0:2]
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
best_c = binary_search_C(X_train, X_val, y_train, y_val)

"""Left side of the graph is where C is low, i.e regularization is strong, hence left side is underfit

Right side of graph is where C is very high, i.e regularization is weak, i.e right side is overfit

## Task 5

Using the above-created function now varies the Gamma parameter with the same binary search techniques as above for the C value which has maximum validation accuracy. Explain, whether the above calculated maximum test accuracy is the optimal test accuracy or there can be a better value of C and Gamma.
"""

def binary_search_gamma(
    X_train: "pd.DataFrame",
    X_val: "pd.DataFrame",
    y_train: "pd.DataFrame",
    y_val: "pd.DataFrame",
    best_c: "float"):
    right = 0.1
    left = 0
    max_acc = 0
    acc = 0.1
    accuracies = []
    Cs = []
    while acc >= max_acc:

        left = right
        right *= 2
        model = SVC(kernel='rbf',gamma = right ,C = best_c)\
            .fit(X_train, y_train.iloc[:, 0])
        max_acc = max(max_acc, acc)
        acc = model.score(X_val, y_val)
        print(left, right, acc)

        accuracies.append(acc)
        Cs.append(right)

    plt.plot(Cs, accuracies, 'o-', label='Forward Exponentiation')
    Cs = []
    accuracies = []

    print("phase2")

    while left <= right:
        mid = (left + right)/2
        model = SVC(kernel='rbf',C = best_c, gamma=mid)\
            .fit(X_train, y_train.iloc[:, 0])
        acc = model.score(X_val, y_val)

        accuracies.append(acc)
        Cs.append(mid)

        print(left, mid ,right, acc)

        if acc < max_acc:
            right = mid - 0.0001
        elif acc > max_acc:
            left = mid + 0.0001
            max_acc = acc
        else:
            break

    plt.plot(Cs, accuracies, 'o-', label="Binary Search")
    plt.title(f"Exponentiation Propagation and Binary Search to find best gamma (C = {best_c})")
    plt.xlabel("Gamma")
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()
    plt.show()



    return mid


binary_search_gamma(X_train, X_val, y_train, y_val, best_c)

"""Binary Search assumes that there is only one maxima of accuracy (and corresponding minima of loss) and will search for that loss minima, it does not take into the account the possibility of multiple minima's (which can be seen by rerunning the last two cells where we get different C and gamma for every run), so this method will give us a local minima, with no guarantee of the minima being the global minima. Hence there may be a better C and gamma value pair which gives better accuracy."""