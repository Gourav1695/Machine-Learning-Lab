# -*- coding: utf-8 -*-
"""assign3_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDdNpNr2rnMrcZlj2zg3hNmsDmr-1QgG

# Assignment 3 Part 2

Machine Learning Lab

Name: Gourav Kumar Shaw

Enrolment No.: 2020CSB010
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
import numpy as np

"""## Task 4

Download the Forest Cover Type dataset (https://www.kaggle.com/uciml/forest-cover-type-dataset) and pre-process the dummy variables to create training, test, and development set. Reduce the train data size if the system unable to process the whole dataset.
"""

import pandas as pd

_FILE_PATH = '/content/drive/MyDrive/ML_DRIVE/Assign_3/covtype.csv'

dataset = pd.read_csv(_FILE_PATH)

dataset

# Let's check for missing values once
# This method calculates the sum of True values (which are treated as 1)
# along each column in the DataFrame. In other words, it counts the number
# of missing values in each column of the DataFrame.
dataset.isnull().sum()

dataset.info();

from sklearn.preprocessing import StandardScaler

def standardize(df: "pd.DataFrame", col_name: "str") -> "pd.DataFrame":
    scaler = StandardScaler()

    df[[col_name]] = pd.DataFrame(
        data=scaler.fit_transform(df[[col_name]]),
        index=df.index,
        columns=[col_name]
    )
    return df

_columns_to_scale = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',
                    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',
                    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',
                    'Horizontal_Distance_To_Fire_Points']

for _col in _columns_to_scale:
   dataset = standardize(dataset, _col)

dataset

# Counter(y): This line uses the Counter class from the collections module to count the occurrences of each unique element in the sequence y.
# It essentially creates a dictionary-like object where the keys are the unique elements in y, and the values are the counts of each unique element.

# For example, if y contains the following values: [1, 2, 2, 3, 1, 1, 2], then before_dist
# will be a dictionary-like object like this: {1: 3, 2: 3, 3: 1}, indicating that there are 3 occurrences of 1, 3 occurrences of 2, and 1 occurrence of 3 in y.

def plot_count(y):
  before_dist = Counter(y)
  print("Before undersampling: ", before_dist)
  plt.xlabel("Count")
  plt.ylabel("Cover Type")
  plt.title("Count of each cover type")
  plt.bar(before_dist.keys(), before_dist.values())

X = dataset.drop('Cover_Type', axis = 1)
y = dataset['Cover_Type']

plot_count(y)

dataset[['Cover_Type']].value_counts()

# NOTE: class imbalance is present but removing it will
# remove the data that cover_type 2 is the most common data in world

dataset = dataset.sample(frac=0.1)

X = dataset.drop('Cover_Type', axis=1)
y = dataset[['Cover_Type']]

# 80% as train
# 10% as validation
# 10% as train

from sklearn.model_selection import train_test_split

X_train, _X, y_train, _y = train_test_split(X, y, train_size=0.8)
X_test,X_val, y_test, y_val = train_test_split(_X, _y, train_size=0.5)

"""## Task 5

Apply multi-class classification in SVM using Forest Cover Type dataset.
"""

print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

# make_pipeline(StandardScaler(), SVC(gamma='auto')):
# This line creates a machine learning pipeline using the make_pipeline function
# from scikit-learn. The pipeline consists of two main components:
# StandardScaler(): This is a preprocessing step that standardizes (scales)
# the features to have a mean of 0 and a standard deviation of 1.
# It's applied to the input data before feeding it to the SVM classifier.
# SVC(gamma='auto'): This creates a Support Vector Machine (SVM)
# classifier with a radial basis function (RBF) kernel. gamma='auto'
#  sets the gamma parameter automatically based on the number of features in the data.

y_train = np.ravel(y_train)
y_test = np.ravel(y_test)
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
# It compares the predicted labels (pred) with the actual labels (y_test)
# to count the number of true positives, true negatives, false positives, and false negatives.
cm = confusion_matrix(y_test, pred)
# The average='macro' parameter specifies that the F1-score should be computed for
# each class and then averaged across all classes. This is suitable for multiclass
# classification problems.
f1 = f1_score(y_test, pred, average='macro')
accuracy = clf.score(X_test, y_test)
print(f"Accuracy = {accuracy}\n")
print(f"F1 Score = {f1}\n")

"""## Task 6

Plot and Analyze the Confusion matrix for the above applied SVM method.
"""

sns.heatmap(cm)
plt.title('HeatMap')
plt.ylabel('Actual Value')
plt.xlabel('Predicted Value')
plt.show()

"""## Task 7

Consider only two features and three classes and train Logistic Regression 3-class Classifier (Any three-class) to show the training and test area in a 2-D plane, using matplotlib.
"""

# sub_X_train: This line creates a new DataFrame sub_X_train by selecting a subset
# of the original feature matrix X. Specifically, it selects all rows (samples) and the first two columns (features) of X.
# The iloc method is used to perform integer-based indexing of rows and columns.

sub_X_train = X.iloc[:, 0:2]
sub_y_train = y
# sub_train: This line creates a new DataFrame sub_train by joining (concatenating) the sub_X_train DataFrame
# (containing the selected features) and the sub_y_train DataFrame (containing the target labels).
# This operation combines the selected features and their corresponding target labels into a single DataFrame for further processing
sub_train = sub_X_train.join(sub_y_train)
# sub_train: This line filters the sub_train DataFrame to include only rows
# where the 'Cover_Type' column contains values that are in the list [1, 2, 3].
# In other words, it keeps only the rows where the 'Cover_Type' is one of these
# three values (1, 2, or 3), effectively creating a subset of the data that is focused on these specific classes.
sub_train = sub_train[sub_train['Cover_Type'].isin([1,2,3])]

sub_X = sub_train.drop('Cover_Type', axis = 1)
sub_y = sub_train['Cover_Type']

plot_count(sub_y)

X_train, X_test, y_train, y_test = train_test_split(sub_X, sub_y, train_size=0.8)

regr = LogisticRegression(multi_class='multinomial')
model = regr.fit(X_train, y_train)
y_pred_test = model.predict(X_test)
y_pred_train = model.predict(X_train)
print(f"Accuracy (Test) = {model.score(X_test, y_test)}")
print(f"Accuracy (Train) = {model.score(X_train, y_train)}")
print(f"F1 Score (Test)= {f1_score(y_test, y_pred_test, average='macro')}")
print(f"F1 Score (Train)= {f1_score(y_train, y_pred_train, average='macro')}")

# The purpose of combining the feature matrix (X_train) and the target labels
# (y_train) into a single DataFrame (df_train) is often for data organization and convenience.
# It allows you to have all the relevant training data in one DataFrame, making it easier to perform various
# data analysis and modeling tasks. However, keep in mind that any modifications made to df_train will also affect
# X_train since they share the same data, so you should use this approach carefully to avoid unintended side effects.

df_train = X_train
df_train['CoverType'] = y_train
df_train

df_test = X_test
df_test['CoverType'] = y_test
df_test

df_pred_test = X_test
df_pred_test['CoverType'] = y_pred_test
df_pred_test

df_pred_train = X_train
df_pred_train['CoverType'] = y_pred_train
df_pred_train

# this code is organizing and splitting the data into separate subsets for each of the three
# classes (1, 2, and 3) in both the training and testing datasets (df_train, df_test, df_pred_test, and df_pred_train).
# This organization can be helpful when we want to analyze or model data separately for each class or perform
# class-specific operations.

df_trains = [df_train[df_train['CoverType'] == i] for i in [1, 2, 3]]
df_tests = [df_test[df_test['CoverType'] == i] for i in [1, 2, 3]]
df_pred_tests = [df_pred_test[df_pred_test['CoverType'] == i] for i in [1, 2, 3]]
df_pred_trains = [df_pred_train[df_pred_train['CoverType'] == i] for i in [1, 2, 3]]

df_trains

def plot_scatter(title, dfs):
  plt.xlabel("Aspect")
  plt.ylabel("Elevation")
  plt.title(title)
  for _df in dfs:
    plt.scatter(_df['Aspect'], _df['Elevation'])

plot_scatter("Train", df_trains)

plot_scatter("Train (Predicted)", df_pred_trains)

plot_scatter("Test", df_tests)

plot_scatter("Test (Predicted)", df_pred_tests)