# -*- coding: utf-8 -*-
"""2020CSB010_Assignment5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOgLbQ-mS2Pw2BGBSmgfV_Li3RZUiMi1

# Assignment 5



Name: Gourav Kumar Shaw

Roll Number: 2020CSB010

 Machine Learning Lab

## Task 1

Download and install TensorFlow from
https://www.tensorflow.org/install/install_sources or using command
`sudo pip install tensorflow` alternatively the Keras library can be used.

## Task 2

Download MNIST dataset (contains class labels for digits 0-9). using
the command:

```python
import tensorflow as tf
data = tf.contrib.learn.datasets.mnist.load_mnist()
```
or
```python
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```
"""

import tensorflow as tf
mnist_data = tf.keras.datasets.mnist.load_data()

mnist_data

"""mnist_data is a Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test).

x_train: uint8 NumPy array of grayscale image data with shapes (60000, 28, 28), containing the training data. Pixel values range from 0 to 255.

y_train: uint8 NumPy array of digit labels (integers in range 0-9) with shape (60000,) for the training data.

x_test: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. Pixel values range from 0 to 255.

y_test: uint8 NumPy array of digit labels (integers in range 0-9) with shape (10000,) for the test data.
"""

import numpy as np

(x_train, y_train), (x_test, y_test) = mnist_data

# mapping 0-255 to 0-1
x_train = np.array([img/255 for img in x_train])
x_test = np.array([img/255 for img in x_test])

assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)

"""## Task 3

Reduce the training size by 1/10 if computation resources are limited.

Define radial basis function (RBF) as

```python
def RBF(x, c, s):
    return np.exp(-np.sum((x-c)**2, axis=1)/(2*s**2))
```
where, x is the actual value, c is centre (assumed as mean) and s is the standard deviation.

Converted 28\*28 image into 32\*32 using rbf and store the new dataset with the labels. Split the dataset as 80% training and 10% validation
and 10% test.
"""

import numpy as np


def RBF(x, c, s):
    return np.exp(-np.sum((x-c)**2, axis=1)/(2*s**2))

# TODO: used simple scaling to upscale the image,
# use rbf to do this in future

# from tensorflow.image import resize

# reshape to convert 28x28 image (assumed greyscale)
# to 28x28x1 (1 denoting only one value per pixel
# [rgb will have three numbers for eg])

# x_train = np.reshape(x_train, (-1, 28, 28, 1))
# x_train = np.array([resize(img, [32, 32]) for img in x_train])
# print(f"x_train shape: {x_train.shape}")

# x_test = np.reshape(x_test, (-1, 28, 28, 1))
# x_test = np.array([resize(img, [32, 32]) for img in x_test])
# print(f"x_test shape: {x_test.shape}")

import pandas as pd
# convert y to categorical
y_train = pd.get_dummies(y_train).to_numpy()
y_test = pd.get_dummies(y_test).to_numpy()

y_train[0:9]

input_shape = x_train[0].shape
num_classes = len(y_train[0])

"""## Task 4

Now run the fully connected network after flattening the data by changing the number the hyper-parameters use adam optimizer(learning rate = 0.001) and categorical cross-entropy loss

| Hidden Layers  | Activation Function  | Hidden Neurons    |
|----------------|----------------------|-------------------|
| 1              | Sigmoid              | \[16\]            |
| 2              | Sigmoid              | \[16,32\]         |
| 3              | Sigmoid              | \[16,32,64\]      |
"""

from tensorflow.keras import Sequential, Input
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt


def train_model(
        activation_function: 'str',
        hidden_neurons: 'list[int]',
        dropout_rate: 'float | None' = None,
        adam_learn_rate=0.001,
        verbose=True):

    model = Sequential()
    model.add(Input(shape=(input_shape)))
    model.add(Flatten())

    for unit in hidden_neurons[::-1]:
        model.add(Dense(unit, activation=activation_function))
        if dropout_rate is not None:
            model.add(Dropout(rate=dropout_rate))

    # softmax as it gives probabilistic value
    # (sum of all the last nodes will be 1)
    model.add(Dense(num_classes, activation='softmax'))

    if verbose:
        model.summary()

    model.compile(optimizer=Adam(learning_rate=adam_learn_rate),
                  loss=CategoricalCrossentropy(),
                  metrics=['accuracy'])

    history = model.fit(x=x_train,
                        y=y_train,
                        validation_split=0.1,
                        epochs=100,
                        callbacks=[
                            EarlyStopping(
                                monitor='val_loss',
                                patience=5,
                                restore_best_weights= True
                            )
                        ],
                        verbose='auto' if verbose else 0
                        )

    return model, history


def plot_history(
        history: "tf.keras.callbacks.History",
        activation_function: 'str',
        hidden_neurons: 'list[int]',
        dropout_rate: 'float | None' = None):

    plt.plot(history.history['loss'], label='Training')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.ylabel('Training Loss')
    plt.xlabel('Epoch')
    plt.legend()

    if dropout_rate is None:
        plt.title(
            f'Loss vs epoch for {activation_function} {hidden_neurons}')
    else:
        plt.title(
            f'Loss vs epoch for {activation_function} {hidden_neurons} dropout {dropout_rate}')

    plt.show()

    plt.plot(history.history['accuracy'], label='Training')
    plt.plot(history.history['val_accuracy'], label='Validation')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')

    if dropout_rate is None:
        plt.title(
            f'Accuracy vs epoch for {activation_function} {hidden_neurons}')
    else:
        plt.title(
            f'Accuracy vs epoch for {activation_function} {hidden_neurons} dropout {dropout_rate}')

    plt.legend()
    plt.show()

result = pd.DataFrame(
    columns=[
        'Hidden Layers',
        'Activation Function',
        'Hidden Neurons',
        'Test Loss',
        'Test Acccuracy'],
)

hidden_neurons = [16]
activation_function = 'sigmoid'

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

activation_function = 'sigmoid'
hidden_neurons = [16, 32]

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

hidden_neurons = [16, 32, 64]
activation_function = 'sigmoid'

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

result

"""## Task 5

Now run the network by changing the number the Activation Function
hyper-parameters:

| Hidden Layers  | Activation Function  | Hidden Neurons    |
|----------------|----------------------|-------------------|
| 3              | Sigmoid              | \[16,32,64\]      |
| 3              | Tanh                 | \[16,32,64\]      |
| 3              | Relu                 | \[16,32,64\]      |
"""

result = pd.DataFrame(
    columns=[
        'Hidden Layers',
        'Activation Function',
        'Hidden Neurons',
        'Test Loss',
        'Test Acccuracy'],
)

hidden_neurons = [16, 32, 64]
activation_function = 'sigmoid'

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

hidden_neurons = [16, 32, 64]
activation_function = 'tanh'

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'

model, history = train_model(activation_function, hidden_neurons)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons)

result

best_activation_fn = result.sort_values(
    by=['Test Acccuracy', 'Test Loss'],
    ascending=[False, True]
    )['Activation Function'].iloc[0]

best_activation_fn

"""## Task 6

Now run the network by changing the number the Dropout hyper-parameters:

| Hidden Layers  | Activation Function  | Hidden Neurons    | Dropout   |
|----------------|----------------------|-------------------|-----------|
| 3              | Relu                 | \[16,32,64\]      |0.9        |
| 3              | Relu                 | \[16,32,64\]      |0.75       |
| 3              | Relu                 | \[16,32,64\]      |0.5        |
| 3              | Relu                 | \[16,32,64\]      |0.25       |
| 3              | Relu                 | \[16,32,64\]      |0.10       |
"""

result = pd.DataFrame(
    columns=[
        'Hidden Layers',
        'Activation Function',
        'Hidden Neurons',
        'Dropout',
        'Test Loss',
        'Test Acccuracy'],
)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'
dropout_val = 0.9

model, history = train_model(activation_function, hidden_neurons,dropout_val)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    dropout_val,
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons, dropout_val)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'
dropout_val = 0.75

model, history = train_model(activation_function, hidden_neurons, dropout_val)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    dropout_val,
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons, dropout_val)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'
dropout_val = 0.5

model, history = train_model(activation_function, hidden_neurons, dropout_val)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    dropout_val,
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons, dropout_val)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'
dropout_val = 0.25

model, history = train_model(activation_function, hidden_neurons, dropout_val)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    dropout_val,
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons, dropout_val)

hidden_neurons = [16, 32, 64]
activation_function = 'relu'
dropout_val = 0.1

model, history = train_model(activation_function, hidden_neurons, dropout_val)
test_loss, test_acc = model.evaluate(x_test, y_test)

print(f"test_loss = {test_loss} test_acc = {test_acc}")

result.loc[len(result.index)] = [
    len(hidden_neurons),
    activation_function,
    str(hidden_neurons),
    dropout_val,
    test_loss,
    test_acc]

plot_history(history, activation_function, hidden_neurons, dropout_val)

result

best_dropout = result.sort_values(
    by=['Test Acccuracy', 'Test Loss'],
    ascending=[False, True]
    )['Dropout'].iloc[0]

best_dropout

"""## Task 7

Plot  the  graph  for  loss  vs  epoch  and  accuracy(train,  validation, accuracy) vs epoch for all the above cases. Point out the logic in the report.

## Task 8

With  the  best    set    hyperparameter  from  above  run  vary  the
Adam  Optimizer  learning  rate  [0.01,  0.001,  0.005,  0.0001,  0.0005]. Print  the  time  to  achieve  the  best  validation  accuracy  (as  reported before from all run)  for all these five run .
"""

print(f"best activation function: {best_activation_fn}")
print(f"best dropout value: {best_dropout}")

import time

result = pd.DataFrame(
    columns=[
        'Hidden Layers',
        'Activation Function',
        'Hidden Neurons',
        'Dropout',
        'Adam Learn Rate',
        'Time Taken',
        'Test Loss',
        'Test Acccuracy'],
)

hidden_neurons = [16, 32, 64]
adam_learn_rates = [0.01,  0.001,  0.005,  0.0001,  0.0005]

for learn_rate in adam_learn_rates:
    start_time = time.time()
    model, _ = train_model(
        activation_function=best_activation_fn,
        hidden_neurons=hidden_neurons,
        adam_learn_rate=learn_rate,
        dropout_rate=best_dropout,
        verbose=False
    )
    end_time = time.time()

    time_taken = end_time - start_time

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

    result.loc[len(result.index)] = [
    len(hidden_neurons),
    best_activation_fn,
    str(hidden_neurons),
    best_dropout,
    learn_rate,
    time_taken,
    test_loss,
    test_acc]

result

best_adam_learn_rate = result.sort_values(
    by=['Test Acccuracy', 'Test Loss'],
    ascending=[False, True]
    )['Adam Learn Rate'].iloc[0]

best_adam_learn_rate

"""## Task 9

Create  five  image(size  28*28)  containing  a  digit  of  your  won
handwriting and test whether  your trained classifier is able to predict it or not.
"""

model, _ = train_model(
    activation_function=best_activation_fn,
    hidden_neurons=hidden_neurons,
    adam_learn_rate=best_adam_learn_rate,
    dropout_rate=best_dropout,
    verbose=True
)

import random

random_idx = random.sample(range(0, len(x_test)), 10)

img_to_predict = np.array([x_test[idx] for idx in random_idx])

categorical_predictions = model.predict(img_to_predict)

for img, cat_pred in zip(img_to_predict, categorical_predictions):
    plt.imshow(img, cmap='gray')
    plt.show()
    pred = np.argmax(cat_pred)
    print(f"predict = {pred}")