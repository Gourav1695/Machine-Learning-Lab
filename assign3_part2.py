# -*- coding: utf-8 -*-
"""assign3_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1krI62o74SXKCJApnpRMWjKCqyskfkIl3

# Assignment 3 Part 2

Machine Learning Lab

Name: Gourav Kumar Shaw

Roll No.:2020CSB010
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Task 4

Download the Forest Cover Type dataset (https://www.kaggle.com/uciml/forest-cover-type-dataset) and pre-process the dummy variables to create training, test, and development set. Reduce the train data size if the system unable to process the whole dataset.
"""

import pandas as pd

_FILE_PATH = '/content/drive/MyDrive/ML_DRIVE/Assign_3/covtype.csv'

cov_df = pd.read_csv(_FILE_PATH)

cov_df

cov_df.columns

from sklearn.preprocessing import StandardScaler

def standardize(df: "pd.DataFrame", col_name: "str") -> "pd.DataFrame":
    scaler = StandardScaler()

    df[[col_name]] = pd.DataFrame(
        data=scaler.fit_transform(df[[col_name]]),
        index=df.index,
        columns=[col_name]
    )
    return df

_columns_to_scale = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',
                    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',
                    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',
                    'Horizontal_Distance_To_Fire_Points']

for _col in _columns_to_scale:
    cov_df = standardize(cov_df, _col)

cov_df

cov_df[['Cover_Type']].value_counts()

# NOTE: class imbalance is present but removing it will
# remove the data that cover_type 2 is the most common data in world

cov_df = cov_df.sample(frac=0.1)

X = cov_df.drop('Cover_Type', axis=1)
y = cov_df[['Cover_Type']]

y.value_counts()

# 80% as train
# 10% as validation
# 10% as train

from sklearn.model_selection import train_test_split

X_train, _X_rest, y_train, _y_rest = train_test_split(X, y, train_size=0.8)
X_val, X_test, y_val, y_test = train_test_split(_X_rest, _y_rest, train_size=0.5)

"""## Task 5

Apply multi-class classification in SVM using Forest Cover Type dataset.
"""

# https://scikit-learn.org/stable/modules/svm.html#svm
# chose LinearSVC cause no mention of kernel to be used
# and LinearSVC is the fastest


# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score

_model = LinearSVC(max_iter=1000000).fit(X_train, y_train.iloc[:, 0])

_y_predict = _model.predict(X_val)
_accuracy = accuracy_score(y_val, _y_predict)
# 'weighted' cause it also takes imbalance of classes into account
_f1 = f1_score(y_val, _y_predict, average='weighted')

print(f"default accuracy = {_accuracy}")
print(f"default f1 = {_f1}")

# hyper parameter tuning

def svm_train(
    X_train: "pd.DataFrame",
    X_val: "pd.DataFrame",
    y_train: "pd.DataFrame",
    y_val: "pd.DataFrame",
    tol=1e-4,
    C=1.0
) -> "LinearSVC":
    '''
    Wrapper Function for sklearn.svm.LinearSVC
    See: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC
    '''

    model = LinearSVC(
        tol=tol,
        C=C,
        max_iter=1000000
    ).fit(X_train, y_train.iloc[:, 0])

    y_predict = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_predict)
    f1 = f1_score(y_val, y_predict, average='weighted')

    return [tol, C, accuracy, f1]

# Test 1: tolerance test
_result = []
_tolerances = [1e-5, 3.3333e-5, 6.6666e-5, 1e-4,
               3.333e-4, 6.666e-4, 1e-3]

for tol in _tolerances:
    _result.append(svm_train(X_train, X_val, y_train, y_val, tol=tol))

_df = pd.DataFrame(
    data=_result,
    columns=["Tolerance", "C", "Accuracy", "f1"]
)
print(_df)
# also sort by ascending Tolerance as less tolerance means
# less training time
best_tol = _df.sort_values(
    ['f1', 'Accuracy', 'Tolerance'], ascending=False
).iloc[0, :]['Tolerance']
print(f"best tolerance = {best_tol}")

# Test 2: C values

_result = []
_c_values = [0.3333, 0.6666, 1, 3.3333, 6.6666, 10]

for c in _c_values:
    _result.append(svm_train(X_train, X_val, y_train, y_val,
                             tol=best_tol, C=c))

_df = pd.DataFrame(
    data=_result,
    columns=["Tolerance", "C", "Accuracy", "f1"]
)

print(_df)
# also sort by ascending C as for same result, smallest C
# means biggest 1/C which means stronger regularization
best_C = _df.sort_values(
    ['f1', 'Accuracy', 'C'], ascending=[False, False, True]
).iloc[0, :]['C']
print(f"best tolerance = {best_C}")

# End Result after using validation dataset for all hyper-parameters

model = LinearSVC(
        tol=best_tol,
        C=best_C,
        max_iter=1000000
    ).fit(X_train, y_train.iloc[:, 0])

y_predict = model.predict(X_test)
accuracy = accuracy_score(y_test, y_predict)
f1 = f1_score(y_test, y_predict, average='weighted')

print(f"Test Accuracy: {accuracy}")
print(f"Test F1: {f1}")

"""## Task 6

Plot and Analyze the Confusion matrix for the above applied SVM method.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

matrix = confusion_matrix(y_test, y_predict)
fig = plt.figure(figsize=(10,10))
sns.heatmap(
    matrix,
    xticklabels=range(1,8),
    yticklabels=range(1,8),
    linewidth=0.5,
    cmap='coolwarm',
    annot=True,
    cbar=True,
    square=True)
plt.title('HeatMap for the model')
plt.ylabel('Actual Value')
plt.xlabel('Predicted Value')
plt.show()

"""## Task 7

Consider only two features and three classes and train Logistic Regression 3-class Classifier (Any three-class) to show the training and test area in a 2-D plane, using matplotlib.
"""

X_train

#  taking first two features

subset_X_train = X_train.iloc[:, 0:2]
subset_y_train = y_train

subset_train = subset_X_train.join(subset_y_train)
subset_train = subset_train[subset_train['Cover_Type'].isin([1,2,3])]

subset_train

model = LinearSVC().fit(
    subset_train.iloc[:, 0:2],
    subset_train.iloc[:, 2]
)

from sklearn.inspection import DecisionBoundaryDisplay

disp = DecisionBoundaryDisplay.from_estimator(
    model,
    subset_train.iloc[:, 0:2],
    xlabel='Elevation',
    ylabel='Aspect',
    alpha=0.5,
    grid_resolution=5000
)

disp.ax_.scatter(
    subset_train['Elevation'],
    subset_train['Aspect'],
    c=subset_train['Cover_Type'],
    s=1
)

subset_X_test = X_test.iloc[:, 0:2]
subset_y_test = y_test

subset_test = subset_X_test.join(subset_y_test)
subset_test = subset_test[subset_test['Cover_Type'].isin([1,2,3])]

disp = DecisionBoundaryDisplay.from_estimator(
    model,
    subset_test.iloc[:, 0:2],
    xlabel='Elevation',
    ylabel='Aspect',
    alpha=0.5,
    grid_resolution=5000
)

disp.ax_.scatter(
    subset_test['Elevation'],
    subset_test['Aspect'],
    c=subset_test['Cover_Type'],
    s=3
)